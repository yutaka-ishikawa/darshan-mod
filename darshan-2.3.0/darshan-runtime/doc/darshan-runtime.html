<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Darshan-runtime installation and usage</title><link rel="stylesheet" type="text/css" href="docbook-xsl.css" /><meta name="generator" content="DocBook XSL Stylesheets V1.78.1" /></head><body><div xml:lang="en" class="article" lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="idp49330208"></a>Darshan-runtime installation and usage</h2></div></div><hr /></div><div class="toc"><p><strong>Table of Contents</strong></p><dl class="toc"><dt><span class="section"><a href="#_introduction">1. Introduction</a></span></dt><dt><span class="section"><a href="#_requirements">2. Requirements</a></span></dt><dt><span class="section"><a href="#_compilation">3. Compilation</a></span></dt><dd><dl><dt><span class="section"><a href="#_cross_compilation">3.1. Cross compilation</a></span></dt></dl></dd><dt><span class="section"><a href="#_environment_preparation">4. Environment preparation</a></span></dt><dd><dl><dt><span class="section"><a href="#_log_directory">4.1. Log directory</a></span></dt><dt><span class="section"><a href="#_instrumentation_method">4.2. Instrumentation method</a></span></dt></dl></dd><dt><span class="section"><a href="#_instrumenting_statically_linked_applications">5. Instrumenting statically-linked applications</a></span></dt><dt><span class="section"><a href="#_instrumenting_dynamically_linked_applications">6. Instrumenting dynamically-linked applications</a></span></dt><dd><dl><dt><span class="section"><a href="#_instrumenting_dynamically_linked_fortran_applications">6.1. Instrumenting dynamically-linked Fortran applications</a></span></dt></dl></dd><dt><span class="section"><a href="#_darshan_installation_recipes">7. Darshan installation recipes</a></span></dt><dd><dl><dt><span class="section"><a href="#_ibm_blue_gene_bg_p_or_bg_q">7.1. IBM Blue Gene (BG/P or BG/Q)</a></span></dt><dt><span class="section"><a href="#_cray_platforms_xe_xc_or_similar">7.2. Cray platforms (XE, XC, or similar)</a></span></dt><dt><span class="section"><a href="#_linux_clusters_using_intel_mpi">7.3. Linux clusters using Intel MPI</a></span></dt><dt><span class="section"><a href="#_linux_clusters_using_mpich">7.4. Linux clusters using MPICH</a></span></dt><dt><span class="section"><a href="#_linux_clusters_using_open_mpi">7.5. Linux clusters using Open MPI</a></span></dt></dl></dd><dt><span class="section"><a href="#_runtime_environment_variables">8. Runtime environment variables</a></span></dt></dl></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="_introduction"></a>1. Introduction</h2></div></div></div><p>This document describes darshan-runtime, which is the instrumentation
portion of the Darshan characterization tool.  It should be installed on the
system where you intend to collect I/O characterization information.</p><p>Darshan instruments applications via either compile time wrappers for static
executables or dynamic library preloading for dynamic executables.  An
application that has been instrumented with Darshan will produce a single
log file each time it is executed.  This log summarizes the I/O access patterns
used by the application.</p><p>The darshan-runtime instrumentation only instruments MPI applications (the
application must at least call <code class="literal">MPI_Init()</code> and <code class="literal">MPI_Finalize()</code>).  However,
it captures both MPI-IO and POSIX file access.  It also captures limited
information about HDF5 and PnetCDF access.</p><p>This document provides generic installation instructions, but "recipes" for
several common HPC systems are provided at the end of the document as well.</p><p>More information about Darshan can be found at the
<a class="ulink" href="http://www.mcs.anl.gov/darshan" target="_top">Darshan web site</a>.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="_requirements"></a>2. Requirements</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
MPI C compiler
</li><li class="listitem">
zlib development headers and library
</li></ul></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="_compilation"></a>3. Compilation</h2></div></div></div><p><strong>Configure and build example. </strong>
</p><pre class="screen">tar -xvzf darshan-&lt;version-number&gt;.tar.gz
cd darshan-&lt;version-number&gt;/darshan-runtime
./configure --with-mem-align=8 --with-log-path=/darshan-logs --with-jobid-env=PBS_JOBID CC=mpicc
make
make install</pre><p>
</p><div class="note" style="margin-left: 0; margin-right: 10%;"><h3 class="title">Detecting file size and alignment</h3><p>You can also add --enable-stat-at-open option to cause the Darshan library
to issue an additional stat() system call on each file the first time that
it is opened on each process.  This allows Darshan to detect the file
alignment (and subsequent unaligned accesses).  It also allows Darshan to
detect the size of files at open time before any I/O is performed.
Unfortunately, this option can cause significant overhead at scale on file
systems such as PVFS or Lustre that must contact every server for a given
file in order to satisfy a stat request.  We therefore disable this
feature by default.</p></div><div class="itemizedlist"><p class="title"><strong>Explanation of configure arguments:</strong></p><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
<code class="literal">--with-mem-align</code> (mandatory): This value is system-dependent and will be
used by Darshan to determine if the buffer for a read or write operation is
aligned in memory.
</li><li class="listitem">
<code class="literal">--with-log-path</code> (this, or <code class="literal">--with-log-path-by-env</code>, is mandatory): This
specifies the parent directory for the directory tree where darshan logs
will be placed
</li><li class="listitem">
<code class="literal">--with-jobid-env</code> (mandatory): this specifies the environment variable that
Darshan should check to determine the jobid of a job.  Common values are
<code class="literal">PBS_JOBID</code> or <code class="literal">COBALT_JOBID</code>.  If you are not using a scheduler (or your
scheduler does not advertise the job ID) then you can specify <code class="literal">NONE</code> here.
Darshan will fall back to using the pid of the rank 0 process if the
specified environment variable is not set.
</li><li class="listitem">
<code class="literal">CC=</code>: specifies the MPI C compiler to use for compilation
</li><li class="listitem">
<code class="literal">--with-log-path-by-env</code>: specifies an environment variable to use to
determine the log path at run time.
</li><li class="listitem">
<code class="literal">--with-log-hints=</code>: specifies hints to use when writing the Darshan log
file.  See <code class="literal">./configure --help</code> for details.
</li><li class="listitem">
<code class="literal">--with-zlib=</code>: specifies an alternate location for the zlib development
header and library.
</li></ul></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_cross_compilation"></a>3.1. Cross compilation</h3></div></div></div><p>On some systems (notably the IBM Blue Gene series), the login nodes do not
have the same architecture or runtime environment as the compute nodes.  In
this case, you must configure darshan-runtime to be built using a cross
compiler.  The following configure arguments show an example for the BG/P system:</p><pre class="screen">--host=powerpc-bgp-linux CC=/bgsys/drivers/ppcfloor/comm/default/bin/mpicc</pre></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="_environment_preparation"></a>4. Environment preparation</h2></div></div></div><p>Once darshan-runtime has been installed, you must prepare a location
in which to store the Darshan log files and configure an instrumentation method.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_log_directory"></a>4.1. Log directory</h3></div></div></div><p>This step can be safely skipped if you configured darshan-runtime using the
<code class="literal">--with-log-path-by-env</code> option.  A more typical configuration uses a static
directory hierarchy for Darshan log
files.</p><p>The <code class="literal">darshan-mk-log-dirs.pl</code> utility will configure the path specified at
configure time to include
subdirectories organized by year, month, and day in which log files will be
placed. The deepest subdirectories will have sticky permissions to enable
multiple users to write to the same directory.  If the log directory is
shared system-wide across many users then the following script should be run
as root.</p><pre class="screen">darshan-mk-log-dirs.pl</pre><div class="note" style="margin-left: 0; margin-right: 10%;"><h3 class="title">A note about log directory permissions</h3><p>All log files written by darshan have permissions set to only allow
read access by the owner of the file.  You can modify this behavior,
however, by specifying the --enable-group-readable-logs option at
configure time.  One notable deployment scenario would be to configure
Darshan and the log directories to allow all logs to be readable by both the
end user and a Darshan administrators group.   This can be done with the
following steps:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
set the --enable-group-readable-logs option at configure time
</li><li class="listitem">
create the log directories with darshan-mk-log-dirs.pl
</li><li class="listitem">
recursively set the group ownership of the log directories to the Darshan
administrators group
</li><li class="listitem">
recursively set the setgid bit on the log directories
</li></ul></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_instrumentation_method"></a>4.2. Instrumentation method</h3></div></div></div><p>The instrumentation method to use depends on whether the executables
produced by your MPI compiler are statically or dynamically linked.  If you
are unsure, you can check by running <code class="literal">ldd &lt;executable_name&gt;</code> on an example
executable.  Dynamically-linked executables will produce a list of shared
libraries when this command is executed.</p><p>Most MPI compilers allow you to toggle dynamic or static linking via options
such as <code class="literal">-dynamic</code> or <code class="literal">-static</code>.  Please check your MPI compiler man page
for details if you intend to force one mode or the other.</p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="_instrumenting_statically_linked_applications"></a>5. Instrumenting statically-linked applications</h2></div></div></div><p>Statically linked executables must be instrumented at compile time.  The
simplest way to do this is to generate an MPI compiler script (e.g. <code class="literal">mpicc</code>)
that includes the link options and libraries needed by Darshan.  Once this
is done, Darshan instrumentation is transparent; you simply compile
applications using the darshan-enabled MPI compiler scripts.</p><p>For MPICH-based MPI libraries, such as MPICH1, MPICH2, or MVAPICH, these
wrapper scripts can be generated automatically.  The following example
illustrates how to produce wrappers for C, C++, and Fortran compilers:</p><pre class="screen">darshan-gen-cc.pl `which mpicc` --output mpicc.darshan
darshan-gen-cxx.pl `which mpicxx` --output mpicxx.darshan
darshan-gen-fortran.pl `which mpif77` --output mpif77.darshan
darshan-gen-fortran.pl `which mpif90` --output mpif90.darshan</pre><p>Please see the Cray recipe in this document for instructions on
instrumenting statically-linked applications on that platform.</p><p>For other MPI Libraries you must manually modify the MPI compiler scripts to
add the necessary link options and libraries.  Please see the
<code class="literal">darshan-gen-*</code> scripts for examples or contact the Darshan users mailing
list for help.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="_instrumenting_dynamically_linked_applications"></a>6. Instrumenting dynamically-linked applications</h2></div></div></div><p>For dynamically-linked executables, darshan relies on the <code class="literal">LD_PRELOAD</code>
environment variable to insert instrumentation at run time.  The executables
should be compiled using the normal, unmodified MPI compiler.</p><p>To use this mechanism, set the <code class="literal">LD_PRELOAD</code> environment variable to the full
path to the Darshan shared library, as in this example:</p><pre class="screen">export LD_PRELOAD=/home/carns/darshan-install/lib/libdarshan.so</pre><p>You can then run your application as usual.  Some environments may require a
special <code class="literal">mpirun</code> or <code class="literal">mpiexec</code> command line argument to propagate the
environment variable to all processes.  Other environments may require a
scheduler submission option to control this behavior.  Please check your
local site documentation for details.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_instrumenting_dynamically_linked_fortran_applications"></a>6.1. Instrumenting dynamically-linked Fortran applications</h3></div></div></div><p>Please follow the general steps outlined in the previous section.  For
Fortran applications compiled with MPICH you may have to take the additional
step of adding
<code class="literal">libfmpich.so</code> to your <code class="literal">LD_PRELOAD</code> environment variable. For example:</p><pre class="screen">export LD_PRELOAD=/path/to/mpi/used/by/executable/lib/libfmpich.so:/home/carns/darshan-install/lib/libdarshan.so</pre><div class="note" style="margin-left: 0; margin-right: 10%;"><h3 class="title">Note</h3><p>The full path to the libfmpich.so library can be omitted if the rpath
variable points to the correct path.  Be careful to check the rpath of the
darshan library and the executable before using this configuration, however.
They may provide conflicting paths.  Ideally the rpath to the  MPI library
would <span class="strong"><strong>not</strong></span> be set by the darshan library, but would instead be specified
exclusively by the executable itself.  You can check the rpath of the
darshan library by running <code class="literal">objdump -x
/home/carns/darshan-install/lib/libdarshan.so |grep RPATH</code>.</p></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="_darshan_installation_recipes"></a>7. Darshan installation recipes</h2></div></div></div><p>The following recipes provide examples for prominent HPC systems.
These are intended to be used as a starting point.  You will most likely have to adjust paths and options to
reflect the specifics of your system.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_ibm_blue_gene_bg_p_or_bg_q"></a>7.1. IBM Blue Gene (BG/P or BG/Q)</h3></div></div></div><p>IBM Blue Gene systems produces static executables by default, uses a
different architecture for login and compute nodes, and uses an MPI
environment based on MPICH.</p><p>The following example shows how to configure Darshan on a BG/P system:</p><pre class="screen">./configure --with-mem-align=16 \
 --with-log-path=/home/carns/working/darshan/releases/logs \
 --prefix=/home/carns/working/darshan/install --with-jobid-env=COBALT_JOBID \
 --with-zlib=/soft/apps/zlib-1.2.3/ \
 --host=powerpc-bgp-linux CC=/bgsys/drivers/ppcfloor/comm/default/bin/mpicc</pre><div class="note" style="margin-left: 0; margin-right: 10%;"><h3 class="title">Rationale</h3><p>The memory alignment is set to 16 not because that is the proper alignment
for the BG/P CPU architecture, but because that is the optimal alignment for
the network transport used between compute nodes and I/O nodes in the
system.  The jobid environment variable is set to <code class="literal">COBALT_JOBID</code> in this
case for use with the Cobalt scheduler, but other BG/P systems may use
different schedulers.  The <code class="literal">--with-zlib</code> argument is used to point to a
version of zlib that has been compiled for use on the compute nodes rather
than the login node.  The <code class="literal">--host</code> argument is used to force cross-compilation
of Darshan.  The <code class="literal">CC</code> variable is set to point to a stock MPI compiler.</p></div><p>Once Darshan has been installed, use the <code class="literal">darshan-gen-*.pl</code> scripts as
described earlier in this document to produce darshan-enabled MPI compilers.
This method has been widely used and tested with both the GNU and IBM XL
compilers.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_cray_platforms_xe_xc_or_similar"></a>7.2. Cray platforms (XE, XC, or similar)</h3></div></div></div><p>The Cray programming environment produces static executables by default,
which means that Darshan instrumentation must be inserted at compile
time.  This can be accomplished by loading a software module that sets
appropriate environment variables to modify the Cray compiler script link
behavior.  This section describes how to compile and install Darshan,
as well as how to use a software module to enable and disable Darshan
instrumentation.</p><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_building_and_installing_darshan"></a>Building and installing Darshan</h4></div></div></div><p>Please set your environment to use the GNU programming environment before
configuring or compiling Darshan.  Although Darshan can be built with a
variety of compilers, the GNU compilers are recommended because it will
produce a Darshan library that is interoperable with the widest range
of compmilers and linkers.  On most Cray systems you can enable the GNU
programming environment with a command similar to "module swap PrgEnv-pgi
PrgEnv-gnu".  Please see your site documentation for information about
how to switch programming environments.</p><p>The following example shows how to configure and build Darshan on a Cray
system using either the GNU programming environment.  Adjust the
--with-log-path and --prefix arguments to point to the desired log file path
and installation path, respectively.</p><pre class="screen">module swap PrgEnv-pgi PrgEnv-gnu
./configure --with-mem-align=8 \
 --with-log-path=/shared-file-system/darshan-logs \
 --prefix=/soft/darshan-2.2.3 \
 --with-jobid-env=PBS_JOBID --disable-cuserid CC=cc
make install
module swap PrgEnv-gnu PrgEnv-pgi</pre><div class="note" style="margin-left: 0; margin-right: 10%;"><h3 class="title">Rationale</h3><p>The job ID is set to <code class="literal">PBS_JOBID</code> for use with a Torque or PBS based scheduler.
The <code class="literal">CC</code> variable is configured to point the standard MPI compiler.</p><p>The --disable-cuserid argument is used to prevent Darshan from attempting to
use the cuserid() function to retrieve the user name associated with a job.
Darshan automatically falls back to other methods if this function fails,
but on some Cray environments (notably the Beagle XE6 system as of March 2012)
the cuserid() call triggers a segmentation fault.  With this option set,
Darshan will typically use the LOGNAME environment variable to determine a
userid.</p></div><p>As in any Darshan installation, the darshan-mk-log-dirs.pl script can then be
used to create the appropriate directory hierarchy for storing Darshan log
files in the --with-log-path directory.</p><p>Note that Darshan is not currently capable of detecting the stripe size
(and therefore the Darshan FILE_ALIGNMENT value) on Lustre file systems.
If a Lustre file system is detected, then Darshan assumes an optimal
file alignment of 1 MiB.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_enabling_darshan_instrumentation"></a>Enabling Darshan instrumentation</h4></div></div></div><p>Darshan will automatically install example software module files in the
following locations (depending on how you specified the --prefix option in
the previous section):</p><pre class="screen">/soft/darshan-2.2.3/share/craype-1.x/modulefiles/darshan
/soft/darshan-2.2.3/share/craype-2.x/modulefiles/darshan</pre><p>Select the one that is appropriate for your Cray programming environment
(see the version number of the craype module in <code class="literal">module list</code>).</p><p>If you are using the Cray Programming Environment version 1.x, then you
must modify the corresponding modulefile before using it.  Please see
the comments at the end of the file and choose an environment variable
method that is appropriate for your system.  If this is not done, then
the compiler may fail to link some applications when the Darshan module
is loaded.</p><p>If you are using the Cray Programming Environment version 2.x then you can
likely use the modulefile as is.  Note that it pulls most of its
configuration from the lib/pkgconfig/darshan-runtime.pc file installed with
Darshan.</p><p>The modulefile that you select can be copied to a system location, or the
install location can be added to your local module path with the following
command:</p><pre class="screen">module use /soft/darshan-2.2.3/share/craype-&lt;VERSION&gt;/modulefiles</pre><p>From this point, Darshan instrumenation can be enabled for all future
application compilations by running "module load darshan".</p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_linux_clusters_using_intel_mpi"></a>7.3. Linux clusters using Intel MPI</h3></div></div></div><p>Most Intel MPI installations produce dynamic executables by default.  To
configure Darshan in this environment you can use the following example:</p><pre class="screen">./configure --with-mem-align=8 --with-log-path=/darshan-logs --with-jobid-env=PBS_JOBID CC=mpicc</pre><div class="note" style="margin-left: 0; margin-right: 10%;"><h3 class="title">Rationale</h3><p>There is nothing unusual in this configuration except that you should use
the underlying GNU compilers rather than the Intel ICC compilers to compile
Darshan itself.</p></div><p>You can use the <code class="literal">LD_PRELOAD</code> method described earlier in this document to
instrument executables compiled with the Intel MPI compiler scripts.  This
method has been briefly tested using both GNU and Intel compilers.</p><div class="note" style="margin-left: 0; margin-right: 10%;"><h3 class="title">Caveat</h3><p>Darshan is only known to work with C and C++ executables generated by the
Intel MPI suite.  Darshan will not produce instrumentation for Fortran
executables.  For more details please check this Intel forum discussion:</p><p><a class="ulink" href="http://software.intel.com/en-us/forums/showthread.php?t=103447&amp;o=a&amp;s=lr" target="_top">http://software.intel.com/en-us/forums/showthread.php?t=103447&amp;o=a&amp;s=lr</a></p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_linux_clusters_using_mpich"></a>7.4. Linux clusters using MPICH</h3></div></div></div><p>Follow the generic instructions provided at the top of this document.  The
only modification is to make sure that the <code class="literal">CC</code> used for compilation is
based on a GNU compiler.  Once Darshan has been installed, it should be
capable of instrumenting executables built with GNU, Intel, and PGI
compilers.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_linux_clusters_using_open_mpi"></a>7.5. Linux clusters using Open MPI</h3></div></div></div><p>Follow the generic instructions provided at the top of this document for
compilation, and make sure that the <code class="literal">CC</code> used for compilation is based on a
GNU compiler.</p><p>Open MPI typically produces dynamically linked executables by default, which
means that you should use the <code class="literal">LD_PRELOAD</code> method to instrument executables
that have been built with Open MPI.  Darshan is only compatible with Open
MPI 1.6.4 and newer.  For more details on why Darshan is not compatible with
older versions of Open MPI, please refer to the following mailing list discussion:</p><p><a class="ulink" href="http://www.open-mpi.org/community/lists/devel/2013/01/11907.php" target="_top">http://www.open-mpi.org/community/lists/devel/2013/01/11907.php</a></p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="_runtime_environment_variables"></a>8. Runtime environment variables</h2></div></div></div><p>The Darshan library honors the following environment variables to modify
behavior at runtime:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
DARSHAN_DISABLE: disables Darshan instrumentation
</li><li class="listitem">
DARSHAN_INTERNAL_TIMING: enables internal instrumentation that will print the time required to startup and shutdown Darshan to stderr at run time.
</li><li class="listitem">
DARSHAN_LOGHINTS: specifies the MPI-IO hints to use when storing the Darshan output file.  The format is a semicolon-delimited list of key=value pairs, for example: hint1=value1;hint2=value2
</li><li class="listitem">
DARSHAN_DISABLE_TIMING: disables the subset of Darshan instrumentation that gathers timing information
</li><li class="listitem">
DARSHAN_MEMALIGN: specifies a value for memory alignment (CP_MEM_ALIGNMENT)
</li><li class="listitem">
DARSHAN_JOBID: specifies the name of the environment variable to use for the job identifier, such as PBS_JOBID
</li><li class="listitem">
DARSHAN_DISABLE_SHARED_REDUCTION: disables the step in Darshan aggregation
in which files that were accessed by all ranks are collapsed into a single
cumulative file record at rank 0.  This option retains more per-process
information at the expense of creating larger log files.
</li></ul></div></div></div></body></html>